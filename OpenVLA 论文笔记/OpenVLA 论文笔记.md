## 1.痛点：
- 以前的模型泛化能力差
- 好的模型都是闭源，难以学习
- 算力太高，高校没有能力去做‘

## 2.创新点
### 1. 独特的“双路融合”视觉编码器架构
![](img/80a549d4ae98862f4cbc173eeaacfb79.png)
这是 OpenVLA 最核心的架构创新。传统的 VLM 或 VLA 通常只使用一个视觉编码器（如 CLIP 或 SigLIP），而 OpenVLA 采用了**混合视觉编码器**。

- **创新点：** OpenVLA 的视觉主干结合了 **SigLIP**（擅长语义理解）和 **DinoV2**（擅长空间几何特征）。
    
- **具体实现：** 输入的图像块（patches）分别通过这两个编码器，生成的特征向量在通道维度（channel-wise）上进行拼接 。
    
- **优势：** 相比于仅使用 SigLIP 或 CLIP 的模型，引入 DinoV2 显著增强了模型的**空间推理能力（spatial reasoning）**，这对机器人精确操作物体至关重要 。实验表明，这种融合架构比单视觉编码器（仅用 SigLIP）的性能高出约 5% 。

### 2. 参数高效微调（LoRA）与量化技术的引入
![](img/fde15b3cec4f5697a510dc63d7f6e4f7.png)
之前的 VLA 模型（如 RT-2-X）通常是封闭的，且微调需要巨大的计算资源。OpenVLA 是首个证明了现代参数高效微调技术在 VLA 上有效的模型。

- **创新点：** 首次将 **LoRA (Low-Rank Adaptation)** 和**模型量化（Quantization）**技术应用于 VLA 领域 。

- **优势：消费级显卡可用：** OpenVLA 可以在消费级 GPU（如 RTX 4090）上通过 4-bit 量化进行推理，且性能相比全精度（bfloat16）几乎没有下降 。 **微调成本低：** 使用 LoRA 使得在单个 A100 GPU 上仅需几小时即可完成对新任务的微调，而无需重新训练整个模型 。相比之下，全量微调需要多卡集群。

- **LoRA：** 用于微调大模型，只调整模型小部分参数。在 OpenVLA 中，全量微调需要更新所有参数，显存压力极大；而 LoRA 只需要训练 **1.4%** 的参数
- **量化：** 降点模型内部的浮点数的精度。原本模型里的权重数字是非常精确的（比如 16 位浮点数 `3.1415926...`），量化就是把它变得“粗糙”一点（比如 4 位整数 `3`） 
    

### 3. 全参数微调的训练策略（打破“冻结”惯例）
![](img/f6dbe5269cab17a881e8c624d769e28c.png)
在以往的 VLM/VLA 研究中（如 Prismatic VLM），通常的做法是冻结视觉编码器，只训练语言模型部分，以保留预训练特征。

- **创新点：** OpenVLA 在训练过程中**没有冻结视觉编码器**，而是对整个模型（包括视觉编码器、投影层和 LLM 主干）进行了全量微调 。
    
- **原理与效果：** 论文发现，预训练的视觉主干可能无法捕捉到机器人操作所需的细粒度空间细节。解冻并微调视觉编码器对于获得高性能至关重要，实验显示微调视觉编码器的成功率（80%）远高于冻结视觉编码器（46.7%）。
## 3.方法
### 1. 输入是什么 (Inputs)

OpenVLA 是一个视觉-语言-动作模型，它的输入包含两个模态：

- **视觉输入 (Image Observation)：** 机器人当前的单张相机视角图像 。在 OpenVLA 的最终设计中，为了平衡计算开销和性能，输入图像的分辨率被统一处理为 $224 \times 224$ 像素 。
    
- **语言指令 (Language Instruction)：** 用户给出的自然语言文本，比如 `"Put eggplant in bowl"`（把茄子放进碗里） 。
    
    
### 2. 输出是什么 (Outputs)

- **动作输出 (Robot Action)：** 模型最终输出的是一个 **7维的连续机器人动作向量 (7D Robot Action)** 。
    
- 这 7 个维度通常代表机器人末端执行器（夹爪）的绝对或相对控制量，具体包括：3 个空间位置偏移量 ($\Delta x, \Delta y, \Delta z$)，3 个旋转角度偏移量 ($\Delta \theta$ 或者 roll, pitch, yaw)，以及 1 个夹爪的开合状态 ($\Delta Grip$) 。
    
---

### 3. 中间的黑盒子分哪几步 (The Black Box Pipeline)

要让一个原本只会“文字接龙”的语言模型（Llama 2）输出物理世界的连续动作，OpenVLA 巧妙地将动作预测转化为了一个视觉-语言序列生成”任务 。具体分为以下关键的四步：

#### 第一步：视觉特征编码 (Vision Encoding)
![](img/53d9b6279efd9624d0f9edf2edd2d279.png)
首先要把图像变成模型能理解的“词”。

- 输入的图像会被切分成许多小的图像块（Patches），分别送入 DinoV2 和 SigLIP 这两个视觉编码器 。
    
- 这两个编码器提取出的特征向量会在通道维度上进行拼接 (concatenated channel-wise)，形成融合了空间几何与语义信息的强大视觉特征 。
    

#### 第二步：模态对齐投影 (MLP Projection)
![](img/975d3a953a62273b87470715a5b386b4.png)
- 视觉特征虽然提取出来了，但语言模型（Llama 2）还不认识它们。因此，模型使用了一个轻量级的两层多层感知机（2-layer MLP projector）作为桥梁 。
    
- 这个投影层将上一步的视觉特征向量，直接映射到 Llama 2 语言模型的词嵌入空间（Language embedding space）中，让图像变成了 LLM 可以直接处理的“视觉 Token” 。
    


#### 第三步：核心魔法 —— 动作离散化 (Action Discretization)
![](img/23431090a702551e195a6a4ece597b55.png)
这是 OpenVLA 能够输出动作的最核心方法。语言模型只能输出离散的词汇（Token），怎么让它输出连续的 7 维物理坐标呢？
 📝 直白翻译
- **第一句：** 借鉴 Brohan 等人的方法，我们将机器人动作的每一个维度分别离散化为 256 个区间（bins）。

- **第二句：** 对于每一个动作维度，我们设定区间宽度，将训练数据中动作的第 1 百分位数到第 99 百分位数之间的范围进行均匀划分 。

 ### **深度大白话解析**

大语言模型（比如 Llama 2）天生只能输出一个个离散的“词”（Token），但机器人的动作是连续的数字（比如向前移动 1.2345 厘米）。怎么让模型准确输出这个动作呢？这就需要用到**“离散化（Discretization）”**和**“分箱（Binning）”**。

**第一部分：切分 256 个“小格子”（Bins）** 想象你有一把尺子，代表机器人在 X 轴上可以移动的完整范围。论文的做法是把这把尺子切成 256 个等长的“小格子”。 每个格子对应一个专属的 Token。如果模型最终预测出“第 128 个格子对应的 Token”，我们就知道机器人大概要移动多少距离了。

**第二部分：为什么是“第 1 到第 99 百分位数（quantile）”？** 这正是这句话最精妙、也最容易让人卡壳的地方！论文紧接着的下一句话解释了这个设计的初衷：为了避免受到极端异常值的干扰 。

我们可以用一个具体的例子来理解：

- 假设在收集的几万条训练数据里，机器人平时在 X 轴上移动的距离绝大多数都在 **-10 厘米到 +10 厘米** 之间。
    
- 但是，现实世界的数据是很脏的。传感器偶尔会抽风，记录下了一个 **+500 厘米** 的“离群值（outlier）”。
    
- 如果按照**绝对的最大值和最小值**来定义这把尺子的长度（即 -10 厘米到 +500 厘米），那这把尺子就会被拉得特别长。把这么长的尺子切成 256 份，每个小格子代表的误差就太大了，动作颗粒度会变得非常粗糙，导致机器人操作极其不精确 。
    
- **“掐头去尾”的聪明策略：** 为了解决这个问题，研究人员把所有动作数据从小到大排好，直接砍掉最小的 1% 和最大的 1%，只取中间 98% 最常见、最正常的动作数据范围（也就是第 1 到第 99 百分位数）

#### 第四步：自回归预测与反解 (Prediction and De-tokenization)
![](img/999e4f1a3524d3c97142152b5da995e9.png)
📝 中文翻译:使用这种离散化方法，对于一个 $N$ 维的机器人动作，我们得到了 $N$ 个取值范围在 $[0 \dots 255]$ 内的离散整数。不幸的是，OpenVLA 语言基座所使用的分词器（Llama 分词器 [10]）仅为微调期间新引入的 token 保留了 100 个“特殊 token（special tokens）”，这对于我们动作离散化所需的 256 个 token 来说太少了。因此，我们再次选择保持简单，并遵循 Brohan 等人 [7] 的方法，直接用我们的动作 token **覆盖（overwriting）** Llama 分词器词汇表中最不常用的 256 个 token（这恰好对应词汇表最后的 256 个 token）。一旦动作被处理成 token 序列，OpenVLA 就会使用标准的**“预测下一个 token（next-token prediction）”目标进行训练，并且==仅在预测的动作 token 上计算交叉熵损失==（cross-entropy loss）。我们将在第 3.4 节讨论实施此训练过程的关键设计决策。接下来，我们将描述用于 OpenVLA 训练的机器人数据集。

- **输入组合：** 模型将投影后的“视觉 Token”，结合固定的提示词模板 `"What should the robot do to {task}? A:"` （机器人该怎么做才能完成某任务？答：），一起送入 70 亿参数的 Llama 2 大脑中 。
    
- **自回归生成：** 像生成文本一样，Llama 2 会通过标准的预测下一个词的目标函数（Next-token prediction objective），依次生成代表动作的 Token 。
    
- **反分词 (Action De-Tokenizer)：** 最后，系统接收到模型输出的这些特殊 Token，再通过反向查表的方式，将离散的 Token 还原回 7 维的连续物理动作 ($\Delta x, \Delta \theta, \Delta Grip$)，发送给机械臂执行 。
### 🔍 深度解析

这段话可以拆解为两个核心技术点：“偷梁换柱”的词表替换术，以及“精准打击”的损失计算。

#### 1. 遇到难题：“坑位”不够了

在上一问中我们知道，连续动作被分成了 256 个格子。为了让大语言模型（Llama 2）能“说出”这 256 个格子，我们需要在它的“字典（词汇表）”里加上 256 个新词。

- **理想情况：** 字典里刚好有几百个空白页，我们直接把新词写进去就行。
    
- **现实情况：** Llama 2 的字典（Tokenizer）只预留了 100 个空白位（Special tokens），完全不够放 256 个动作词。如果是扩充词表，就需要修改模型底层的 Embedding 矩阵维度，这会带来额外的计算和对齐麻烦。
    

#### 2. 聪明的 Hack（暴力解法）：偷梁换柱

既然加不进去，那就“占山为王”**。

- 语言模型的词表里总有一些极其生僻、或者基本用不到的词（通常排在词表的最后面）。
    
- 论文直接把 Llama 词表里排名最后的 256 个没用的词“抹掉”，赋予它们全新的物理意义——“动作的 256 个刻度”。
    
- 这样一来，模型的结构和参数维度一点都不用改，完美实现了无缝接入。这种做法简单粗暴但极其有效。
    

#### 3. 训练核心：预测下一个词 + 局部算 Loss（红底高亮部分的精髓）

这句话 (`evaluating the cross-entropy loss on the predicted action tokens only`) 揭示了 VLA 训练时最关键的代码实现细节之一。

- **正常的 LLM 训练：** 给一段话 `"What should the robot do to put eggplant in bowl? A: Move hand down"`，模型会在每一个词上计算损失（Loss）。比如它预测 `What` 错成了 `How`，就会被扣分。
    
- **OpenVLA 的训练：** 我们的目标不是让模型学会怎么用流利的英语复述提示词，而是让它**做动作**。所以，前半部分的指令提示词（Prompt）部分，模型爱怎么预测怎么预测，**不算 Loss，不惩罚它**。
    
- **只盯着动作打分：** 只有当模型开始输出代表动作的 token（也就是那 256 个被偷梁换柱的词）时，系统才开始计算交叉熵损失（Cross-entropy loss）。这样能强迫模型把所有的“学习精力”都集中在如何精准预测物理动作上，而不是去学语法。

## 训练流程总结：
### 训练过程：
**输入与模态对齐**：系统接收图像和文本指令 。文本直接转化为文本 Token ；图像则经过 Vision Encoder (DinoV2 + SigLIP) 提取特征，再经过 MLP 投影层，转化为语言模型能理解的“图像 Token” 。
**ransformer 处理**：图像 Token 和文本 Token 拼接后，送入 Llama 大模型中进行自回归推理
**动作离散化与输出**：模型输出预测的“动作 Token” 。系统将真实世界连续的机器人动作在 1% 到 99% 的有效数据范围内均匀划分为了 256 个区间（Bins） ，并用 256 个特殊的 Token 来表示这些区间刻度 。
**损失计算与参数更新**：仅提取模型预测出的“动作 Token”，将其与真实动作对应的目标 Token 进行对比，计算交叉熵损失（Cross-Entropy Loss） 。最后，通过反向传播更新模型参数。

## 证明：
### 1. 用了什么数据集 (Dataset)？
OpenVLA 的预训练底座使用了目前机器人领域极具规模的 **Open X-Embodiment (OpenX) 数据集** 。 作者团队在原始数据的基础上进行了严格的筛选和清洗，只保留了包含第三人称视角和单臂控制的数据，最终用于训练的数据集包含了高达 **97 万条真实世界的机器人运行轨迹 (970k robot episodes)** 。
### 2.对比了哪些baseline？
为了证明自己的强大，论文非常有针对性地挑选了两批业内顶尖的“假想敌”：

- **通用控制能力（开箱即用）：** 对比了从头训练的 Transformer 策略 **RT-1-X (35M参数)** 和 **Octo (93M参数)**，以及目前表现最强、拥有 550 亿参数的闭源大模型 **RT-2-X** 。
    
- **微调能力（学习新技能）：** 在针对新任务微调的实验中，它还对比了目前非常流行的、极其擅长模仿学习的 **Diffusion Policy** 模型 。
## 代价
### 1. 模型复杂度与巨大的显存占用

为了获得强大的“常识”和泛化能力，OpenVLA 牺牲了模型的轻量化。

- **参数量庞大**：OpenVLA 是一个 70 亿（7B）参数的庞然大物 。相比之下，之前的开源基准模型 RT-1-X 只有 3500 万（35M）参数，Octo 只有 9300 万（93M）参数 。
    
- **显存消耗极高**：在标准的 16 位浮点数（bfloat16）精度下，部署 OpenVLA 进行推理需要消耗 16.8 GB 的 GPU 显存 。这意味着它无法在普通的低端显卡上运行。
    
- **量化带来的风险**：虽然通过 4-bit 量化可以将显存压缩到 7.0 GB ，但论文发现，如果使用常规的 8-bit 量化，不仅显存没降到最低（10.2 GB），反而会因为额外的计算开销导致推理速度变慢，最终导致机器人在物理世界的任务成功率从 71.3% 暴跌到 58.1% 。
    
### 2. 推理速度的瓶颈

大模型的计算复杂度直接拖慢了机器人的反应速度。

- **顶配显卡勉强及格**：即使在顶级的消费级显卡 RTX 4090 上，OpenVLA 的推理速度也仅为大约 6 Hz（每秒 6 次动作） 。
    
- **无法应对高频控制**：论文在“局限性 (Limitations)”章节明确指出，目前的推理吞吐量对于高频控制任务来说是一个致命缺陷 。例如，目前前沿的 ALOHA 双臂协作机器人系统需要 50 Hz 的控制频率，OpenVLA 的反应速度根本跟不上，这也限制了它在需要高度灵巧、双臂快速操作任务上的应用 。

### 3. 极其昂贵的训练与微调成本 

相比于从头训练一个小模型，OpenVLA 的训练时间和硬件要求呈指数级增加。

- **预训练堪比烧钱**：为了训练这个 7B 的模型，作者团队动用了 64 张 A100 GPU 组成的集群，连续训练了 14 天（总计消耗了 21,500 个 A100 显卡小时） 。
    
- **收敛极慢**：普通的语言模型微调通常只需要跑 1 到 2 个 Epoch 。但由于机器人动作预测的特殊性，OpenVLA 必须在完整数据集上狂跑 27 个 Epoch 才能达到良好的性能 。
    
- **微调依然沉重**：如果不使用 LoRA 等技术进行“全参数微调”，即使只针对一个新任务微调，也需要 8 张 A100 GPU 跑 5 到 15 个小时 。虽然 LoRA 把门槛降到了单张 A100 跑 10 到 15 小时 ，但比起只需几分钟就能微调完的小模型，时间成本依然很高。
    
### 4. 模态和结构的妥协

为了强行套用 LLM 的“词汇预测”架构，它在输入和动作控制上做了简化。

- **缺乏记忆与多模态**：OpenVLA 目前只能接收“单张静态图片”作为输入，没有观察历史记录（Observation History），也没有引入机器人的本体感受数据（如机械臂当前的关节角度和力度） 。这使得它在需要感受物理反馈或需要记住上一步状态的复杂任务中处于劣势。
## Limitation
### 1. 只有“单张静态视觉”，缺乏“记忆”与“触觉”

- **论文原话：** 模型目前仅支持单张图像作为观察输入 。它没有引入观测历史（observation history），也没有结合机器人的本体感受输入（proprioceptive inputs，比如关节力度、角度等）。在“空间维度”上，单张图片（单视角）往往存在**视觉盲区（遮挡）**，像Pi0模型可以多视角拍摄机器人画面。
    
- **必定失效的场景：** * **需要物理反馈的任务：** 比如把一根紧绷的插销插进孔里（Peg-in-hole），这种任务眼睛看不太清，必须依靠机械臂感受到的“阻力”来微调。
    
    - **需要短期记忆的任务：** 比如“把你刚才打开的那个抽屉关上”，因为它只看当前这一帧画面，根本不记得刚才发生过什么。
        

### 2. 反射弧太长，做不了高频和灵巧动作

- **论文原话：** 对于像 ALOHA 这种运行频率高达 50Hz 的控制系统，OpenVLA 目前的推理吞吐量（速度）是个致命瓶颈 。在面对相对狭窄但需要高度灵巧的任务时，Diffusion Policy 生成的轨迹仍然比它更平滑、更精确。
    
- **必定失效的场景：**
    
    - **动态抓取：** 比如接住空中抛过来的球，或者在流水线上抓取快速移动的物体。大模型预测动作需要时间，等它算出结果，球早掉地上了。
        
    - **复杂的双臂协同：** 比如像人类一样双手灵巧地系鞋带 。
        

### 3. 可靠性尚未达到“包治百病”的程度

- **论文原话：** 尽管 OpenVLA 优于之前的通用策略，但在测试任务中它的绝对成功率通常仍然低于 90%，还没有提供非常高的可靠性 。
    
- **可能表现不佳的场景：** 工业流水线或医疗手术环境。在这些场景下，容错率极低（通常要求 99.9% 以上的成功率），低于 90% 的成功率意味着它目前还只能在实验室或者宽容度高的家政场景里做研究，没法直接落地进厂打工。
    

### 4. 存在“真机到仿真”的领域鸿沟 (Real-to-Sim Gap)

- **论文原话：** OpenVLA 完全是在真实的机器人数据上进行预训练的，没有使用任何仿真数据 。因此，由于真实环境和仿真环境/动力学之间的领域差距，在仿真机器人任务上微调该模型的效果，可能不如在真实世界任务上微调那么好 。
    
- **可能表现不佳的场景：** 如果一个实验室没有真机，想完全在纯虚拟引擎（如 Isaac Sim）里用 OpenVLA 跑一套复杂的仿真业务，它的表现可能会打折扣。